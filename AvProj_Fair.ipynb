{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import & installation"
      ],
      "metadata": {
        "id": "GLIfv6y_VjeJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb-dv6YiT5ec",
        "outputId": "2070bcf2-e4ae-47ea-e810-67eb97ff426a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-metar\n",
            "  Downloading python-metar-1.4.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.5)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Building wheels for collected packages: python-metar\n",
            "  Building wheel for python-metar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-metar: filename=python_metar-1.4.0-py3-none-any.whl size=16926 sha256=c7efeb736c88c1d2bd99137f073274fa7d52cf1d0c479078a1f1d02ae4e84b92\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/c7/33/370bed0725fd1aab6f731fd77dadfc7b66bdb6998909b7d8d0\n",
            "Successfully built python-metar\n",
            "Installing collected packages: python-metar\n",
            "Successfully installed python-metar-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-metar xgboost lightgbm scikit-learn pandas numpy matplotlib seaborn requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from metar import Metar\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "import re\n",
        "\n",
        "\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "AplNjyiEUmNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# url_str = \"https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?network=IN__ASOS&station=\" + input(\"Enter ICAO code\\n\") +\"&data=metar&year1=2025&month1=8&day1=2&year2=2025&month2=8&day2=2&tz=Etc%2FUTC&format=onlycomma&latlon=no&elev=no&missing=null&trace=0.0001&direct=no&report_type=3&report_type=4\"\n",
        "# print(url_str)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "muUexC7EYRSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?network=IN__ASOS&station=VIAR&data=metar&year1=2018&month1=1&day1=1&year2=2023&month2=12&day2=12&tz=Etc%2FUTC&format=onlycomma&latlon=no&elev=no&missing=null&trace=0.0001&direct=no&report_type=3&report_type=4\"\n",
        "filename = \"metar_data.csv\"\n",
        "response = requests.get(url)\n",
        "with open(filename, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "print(\"Data downloaded and saved as\", filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbfcoIChUwsq",
        "outputId": "d22e3ce4-37b5-413f-876c-16198d99e7b9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data downloaded and saved as metar_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('metar_data.csv')\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nColumn info:\")\n",
        "print(df.info())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF9H22LUU7GR",
        "outputId": "25f1174a-5105-4e22-9678-5f0d77407afc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (94732, 3)\n",
            "\n",
            "Column info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 94732 entries, 0 to 94731\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   station  94732 non-null  object\n",
            " 1   valid    94732 non-null  object\n",
            " 2   metar    94732 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 2.2+ MB\n",
            "None\n",
            "\n",
            "First few rows:\n",
            "  station             valid                                              metar\n",
            "0    VIAR  2018-01-01 00:00  VIAR 010000Z 00000KT 0300 R34/0700 FG NSC 06/0...\n",
            "1    VIAR  2018-01-01 00:30  VIAR 010030Z 00000KT 0300 R34/1000 FG NSC 06/0...\n",
            "2    VIAR  2018-01-01 01:00  VIAR 010100Z 00000KT 0300 R34/0325 FG NSC 07/0...\n",
            "3    VIAR  2018-01-01 01:30  VIAR 010130Z 00000KT 0400 R34/0450 FG NSC 06/0...\n",
            "4    VIAR  2018-01-01 02:00  VIAR 010200Z 00000KT 0400 R34/1200 FG NSC 06/0...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#General Functions"
      ],
      "metadata": {
        "id": "NurIxQEmf9j_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_metar_string(raw_metar: str) -> str:\n",
        "    \"\"\"\n",
        "    Cleans METAR string to remove correction/amendment flags and unsupported fields.\n",
        "    \"\"\"\n",
        "    metar = raw_metar.strip()\n",
        "\n",
        "    # Remove METAR/SPECI, COR, AMD prefixes anywhere at start\n",
        "    metar = re.sub(r'^(METAR|SPECI)?\\s*(COR|AMD)?\\s*', '', metar, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove malformed RVRs like RMID/2000 or RM34/P2000\n",
        "    metar = re.sub(r'R[M]?[A-Z0-9]{2,4}/P?\\d{3,4}', '', metar)\n",
        "\n",
        "    # Remove unsupported VV/// (vertical visibility unknown)\n",
        "    metar = re.sub(r'VV///', '', metar)\n",
        "\n",
        "    # Remove malformed runway groups that the parser can't handle\n",
        "    metar = re.sub(r'R\\d{2}/\\d{4}', '', metar)\n",
        "\n",
        "    # Remove double slashes (extra spacing)\n",
        "    metar = re.sub(r'\\s{2,}', ' ', metar)\n",
        "\n",
        "    return metar.strip()\n",
        "\n",
        "def parse_metar_comprehensive(metar_string):\n",
        "    \"\"\"\n",
        "    Parse METAR string for aviation-related weather and ML feature extraction.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        cleaned_metar = clean_metar_string(metar_string)\n",
        "        report = Metar.Metar(cleaned_metar)\n",
        "        parsed_data = {\n",
        "            # 'station': report.station_id,\n",
        "            # 'datetime': report.time if hasattr(report, 'time') else None,\n",
        "            # 'raw_metar': metar_string,\n",
        "            'wind_gust': report.wind_gust.value() if hasattr(report, 'wind_gust') and report.wind_gust else None,\n",
        "            'wind_spd_kt': report.wind_speed.value() if hasattr(report, 'wind_speed') and report.wind_speed else None,\n",
        "            'wind_dir_deg': report.wind_dir.value() if hasattr(report, 'wind_dir') and report.wind_dir else None,\n",
        "            'vis_m': report.vis.value() if hasattr(report, 'vis') and report.vis else None,\n",
        "            'temp_c': report.temp.value() if hasattr(report, 'temp') and report.temp else None,\n",
        "            'dewpt_c': report.dewpt.value() if hasattr(report, 'dewpt') and report.dewpt else None,\n",
        "            'pressure_hpa': report.press.value() if hasattr(report, 'press') and report.press else None,\n",
        "            'sky': report.sky if hasattr(report, 'sky') and report.sky else [],\n",
        "        }\n",
        "\n",
        "        # Calculate dew point spread (fix field names)\n",
        "        if parsed_data['temp_c'] is not None and parsed_data['dewpt_c'] is not None:\n",
        "            parsed_data['dewpoint_spread'] = parsed_data['temp_c'] - parsed_data['dewpt_c']\n",
        "        else:\n",
        "            parsed_data['dewpoint_spread'] = None\n",
        "\n",
        "        # Extract weather phenomena (winter conditions)\n",
        "        weather_phenomena = str(report.weather) if hasattr(report, 'weather') and report.weather else \"\"\n",
        "        # Tokenize weather phenomena\n",
        "        weather_tokens = weather_phenomena.split()\n",
        "\n",
        "        weather_phenomena = str(report.weather) if report.weather else \"\"\n",
        "        parsed_data['has_fog'] = 1 if 'FG' in weather_phenomena else 0\n",
        "        parsed_data['has_mist'] = 1 if 'BR' in weather_phenomena else 0\n",
        "        parsed_data['has_haze'] = 1 if 'HZ' in weather_phenomena else 0\n",
        "        parsed_data['has_rain'] = 1 if 'RA' in weather_phenomena else 0\n",
        "\n",
        "        parsed_data['weather_phenomena'] = weather_phenomena\n",
        "\n",
        "        # Extract raw representation of sky tuple list as 'sky_conditions'\n",
        "        # sky_conditions = str(report.sky) if hasattr(report, 'sky') and report.sky else \"\"\n",
        "        # parsed_data['sky_conditions'] = sky_conditions\n",
        "\n",
        "        # Extract ceiling height from first OVC/BKN layer (if present)\n",
        "        ceiling_height = None\n",
        "        if hasattr(report, 'sky') and report.sky:\n",
        "            for sky_layer in report.sky:\n",
        "                if sky_layer[0] in ['OVC', 'BKN']:\n",
        "                    ceiling_height = sky_layer[1].value() if sky_layer[1] else None\n",
        "                    break\n",
        "        parsed_data['ceiling_height_ft'] = ceiling_height\n",
        "\n",
        "        # IFR/VFR flight category, using correct keys\n",
        "        vis_m = parsed_data['vis_m']\n",
        "        ceiling_ft = parsed_data['ceiling_height_ft']\n",
        "        # Flight category determination\n",
        "        if vis_m is not None and vis_m < 1600 or (ceiling_ft is not None and ceiling_ft < 500):\n",
        "            flight_category = 'LIFR'\n",
        "        elif vis_m is not None and vis_m < 4800 or (ceiling_ft is not None and ceiling_ft < 1000):\n",
        "            flight_category = 'IFR'\n",
        "        elif vis_m is not None and vis_m < 8000 or (ceiling_ft is not None and ceiling_ft < 3000):\n",
        "            flight_category = 'MVFR'\n",
        "        else:\n",
        "            flight_category = 'VFR'\n",
        "        parsed_data['flight_category'] = flight_category\n",
        "\n",
        "        return parsed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing METAR: {metar_string}\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "xXIoTOZjVAKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_dataset(df):\n",
        "    \"\"\"\n",
        "    Analyzes the dataset for missing values, data types, and basic statistics.\n",
        "\n",
        "    Args:\n",
        "        df: pandas DataFrame\n",
        "    \"\"\"\n",
        "    print(\"Dataset Shape:\")\n",
        "    print(df.shape)\n",
        "\n",
        "    # print(\"\\nData Types:\")\n",
        "    # print(df.dtypes)\n",
        "\n",
        "    print(\"\\nMissing Values Count out of: \"+str(len(df)))\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    print(\"\\nPercentage of Missing Values:\")\n",
        "    print((df.isnull().sum() / len(df)) * 100)\n",
        "\n",
        "    # print(\"\\nBasic Statistics:\")\n",
        "    # display(df.describe(include='all'))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2kfJLkpL66Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Optional, Union\n",
        "\n",
        "def parse_metar_clouds(text: str) -> List[Union[Tuple[str, int, Optional[str]], str]]:\n",
        "    \"\"\"\n",
        "    Parses cloud and weather trend info from a METAR string.\n",
        "\n",
        "    Returns a list of:\n",
        "    - Tuples: (cloud cover, base altitude in feet, CB if present)\n",
        "    - Strings: \"NSC\" or \"NOSIG\" if present\n",
        "    \"\"\"\n",
        "    pattern = r'\\b(FEW|SCT|BKN|OVC)(\\d{3})(CB)?\\b'\n",
        "    cloud_matches = re.findall(pattern, text)\n",
        "\n",
        "    results = []\n",
        "    for cover, height_str, cb in cloud_matches:\n",
        "        height = int(height_str) * 100  # Convert to feet\n",
        "        results.append((cover, height, cb if cb else None))\n",
        "\n",
        "    # Look for NSC and NOSIG separately\n",
        "    if re.search(r'\\bNSC\\b', text):\n",
        "        results.append(\"NSC\")\n",
        "\n",
        "    if re.search(r'\\bNOSIG\\b', text):\n",
        "        results.append(\"NOSIG\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "vbMnBZYuhN9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sky_features(metar_string):\n",
        "    \"\"\"\n",
        "    Extracts cloud layers and overcast information, and NSC/NOSIG from a METAR string.\n",
        "    \"\"\"\n",
        "    parsed_results = parse_metar_clouds(metar_string)\n",
        "    layer_data = {}\n",
        "    cloud_layers = [item for item in parsed_results if isinstance(item, tuple)]\n",
        "\n",
        "    for i, layer in enumerate(cloud_layers):\n",
        "        layer_data[f'layer{i+1}_cover'] = layer[0]\n",
        "        layer_data[f'layer{i+1}_height_ft'] = layer[1]\n",
        "        if layer[2] == 'CB':\n",
        "            layer_data[f'layer{i+1}_cb'] = 1\n",
        "        else:\n",
        "            layer_data[f'layer{i+1}_cb'] = 0\n",
        "\n",
        "    # Initialize columns for up to 4 layers in case they are not present\n",
        "    for i in range(1, 5):\n",
        "        layer_data.setdefault(f'layer{i}_cover', None)\n",
        "        layer_data.setdefault(f'layer{i}_height_ft', None)\n",
        "        layer_data.setdefault(f'layer{i}_cb', 0)\n",
        "\n",
        "\n",
        "    # Check for NSC and NOSIG\n",
        "    layer_data['nsc'] = 0\n",
        "    layer_data['nosig'] = 0\n",
        "    if \"NSC\" in parsed_results:\n",
        "        layer_data['layer1_cover'] = 'NSC'\n",
        "        layer_data['nsc'] = 1\n",
        "    if \"NOSIG\" in parsed_results:\n",
        "        layer_data['nosig'] = 1\n",
        "\n",
        "    # Check for overcast (OVC) in any layer\n",
        "    layer_data['overcast'] = 0\n",
        "    for layer in cloud_layers:\n",
        "        if layer[0] == 'OVC':\n",
        "            layer_data['overcast'] = 1\n",
        "            break\n",
        "\n",
        "    return layer_data"
      ],
      "metadata": {
        "id": "yVaolWoYhPCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_lag_features(df, cols,lags=[1,2,3,6]):\n",
        "    for col in cols:\n",
        "        for lag in lags:\n",
        "            df[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
        "    return df"
      ],
      "metadata": {
        "id": "VUT6qYkxhiFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forecast_temperature(model, latest_data: pd.DataFrame, features: list, horizon=2):\n",
        "    \"\"\"\n",
        "    model: trained RF model\n",
        "    latest_data: dataframe with the most recent data points (at least enough for lags)\n",
        "    features: list of features the model was trained on\n",
        "    horizon: how many steps ahead to forecast (1=next 30min, 2=next 1h, etc.)\n",
        "    \"\"\"\n",
        "    preds = []\n",
        "    # Ensure we have enough data for the required lags and the forecast horizon\n",
        "    required_length = max(6, horizon) + 1 # Max lag is 6, plus horizon, plus one for the current row\n",
        "    temp_data = latest_data.tail(required_length).copy()\n",
        "\n",
        "\n",
        "    for step in range(horizon):\n",
        "        # Prepare the features for the current step's prediction\n",
        "        X_latest = temp_data[features].iloc[[-1]]\n",
        "\n",
        "        # Predict the next step's temperature\n",
        "        y_pred = model.predict(X_latest)[0]\n",
        "        preds.append(y_pred)\n",
        "\n",
        "        # Update the temp_c column with the prediction for the next step\n",
        "        # This is necessary to generate lags for subsequent predictions in the horizon\n",
        "        next_row_index = temp_data.index[-1] + 1\n",
        "        next_row = pd.Series(index=temp_data.columns)\n",
        "        next_row['temp_c'] = y_pred # Update with prediction\n",
        "        next_row['valid'] = temp_data['valid'].iloc[-1] + timedelta(minutes=30) # Increment time\n",
        "\n",
        "        # Update cyclic features for the next time step\n",
        "        next_valid_dt = pd.to_datetime(next_row['valid'])\n",
        "        next_row['hour'] = next_valid_dt.hour\n",
        "        next_row['minute'] = next_valid_dt.minute\n",
        "        next_row['hour_fraction'] = (next_row['hour'] + next_row['minute'] / 60) / 24\n",
        "        next_row['time_x'] = np.cos(2 * np.pi * next_row['hour_fraction'])\n",
        "        next_row['time_y'] = np.sin(2 * np.pi * next_row['hour_fraction'])\n",
        "        next_row['day_of_year'] = next_valid_dt.dayofyear\n",
        "        next_row['day_x'] = np.cos(2 * np.pi * next_row['day_of_year'] / 365)\n",
        "        next_row['day_y'] = np.sin(2 * np.pi * next_row['day_of_year'] / 365)\n",
        "\n",
        "\n",
        "        # Shift lag features in the next row based on current temp_data\n",
        "        for lag in [1, 2, 3, 6]:\n",
        "            if f'temp_c_lag{lag}' in temp_data.columns:\n",
        "                # Find the index in temp_data that corresponds to the value needed for the lag\n",
        "                lag_index = -lag # Negative index from the end of temp_data\n",
        "                if abs(lag_index) <= len(temp_data):\n",
        "                    next_row[f'temp_c_lag{lag}'] = temp_data['temp_c'].iloc[lag_index]\n",
        "                else:\n",
        "                     # If not enough data in temp_data for this lag, use the last known value\n",
        "                     next_row[f'temp_c_lag{lag}'] = temp_data['temp_c'].iloc[0]\n",
        "\n",
        "        # Append the new row with the prediction and updated features for the next step\n",
        "        next_row_df = pd.DataFrame([next_row], index=[next_row_index])\n",
        "        temp_data = pd.concat([temp_data, next_row_df])\n",
        "        temp_data = temp_data.tail(required_length) # Keep only the latest required rows\n",
        "\n",
        "\n",
        "    return preds"
      ],
      "metadata": {
        "id": "FUnj5YzIhSZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0CSuvl1rhSPi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xmvWByhdhxcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extracting from Drive"
      ],
      "metadata": {
        "id": "usKgJ8AMDqo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tB9gR8uDB5e",
        "outputId": "57b45062-6908-4150-d041-7022c003fda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/Colab_CSV/avproj_dataset.csv'\n",
        "df2 = pd.read_csv(file_path)\n",
        "print(f\"DataFrame loaded from: {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oo_kzcbDuod",
        "outputId": "771d3fce-6549-4e26-9d48-6ad838ef54a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame loaded from: /content/drive/My Drive/Colab_CSV/avproj_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "cYcA9uvTiqJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Final Model"
      ],
      "metadata": {
        "id": "nVRL4Bv6OBUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Temp"
      ],
      "metadata": {
        "id": "BziT5HjwjOiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###RF"
      ],
      "metadata": {
        "id": "JQxpsQLfivFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model from Google Drive\n",
        "file_path = '/content/drive/My Drive/Colab_Models/Aviation_Models/rf_temp_forecast.pkl'\n",
        "rf_final_temp = joblib.load(file_path)\n",
        "print(f\"Model loaded from: {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5pFR4zOKl7B",
        "outputId": "6ef0fdc8-d72c-43a0-9339-3606468d82e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from: /content/drive/My Drive/Colab_Models/Aviation_Models/rf_temp_forecast.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###XGB"
      ],
      "metadata": {
        "id": "hazk_5LVDPcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved XGBoost model from Google Drive\n",
        "file_path = '/content/drive/My Drive/Colab_Models/Aviation_Models/xgb_temp_forecast.pkl'\n",
        "xgb_final_temp = joblib.load(file_path)\n",
        "print(f\"XGBoost model loaded from: {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXbsxJB1DhOk",
        "outputId": "1b617826-da54-45ab-af40-c430611d60f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost model loaded from: /content/drive/My Drive/Colab_Models/Aviation_Models/xgb_temp_forecast.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mock Prediction"
      ],
      "metadata": {
        "id": "1aBEuvdJja1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mock Pred Temp"
      ],
      "metadata": {
        "id": "ML8ag11gja1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Input and Preprocessing"
      ],
      "metadata": {
        "id": "UiFKs0NRja1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "date_str = input(\"Enter the date (YYYY-MM-DD): \")\n",
        "time_str = input(\"Enter the time (HH:MM:SS): \")\n",
        "input_datetime_str = f\"{date_str} {time_str}\"\n",
        "\n",
        "try:\n",
        "    input_datetime = datetime.strptime(input_datetime_str, '%Y-%m-%d %H:%M:%S')\n",
        "    print(f\"You entered: {input_datetime}\")\n",
        "except ValueError:\n",
        "    print(\"Invalid date or time format. Please use YYYY-MM-DD for date and HH:MM:SS for time.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f1d3f83-1a9b-4178-d711-e34f8d41e75f",
        "id": "5jNy6K8_ja1u"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the date (YYYY-MM-DD): 2025-09-10\n",
            "Enter the time (HH:MM:SS): 17:30:00\n",
            "You entered: 2025-09-10 17:30:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19bb1f7-8a9e-47c7-d677-77ab36e515d9",
        "id": "XKxwtdG9ja1v"
      },
      "source": [
        "from datetime import timedelta\n",
        "\n",
        "# Use the input_datetime from the previous cell (assuming it's defined)\n",
        "# Calculate the start date (one day before the input date)\n",
        "start_datetime = input_datetime - timedelta(days=1)\n",
        "\n",
        "# Calculate the end date (one day after the input date)\n",
        "end_datetime = input_datetime + timedelta(days=1)\n",
        "\n",
        "\n",
        "# Extract year, month, and day for the start and end dates\n",
        "year1 = start_datetime.year\n",
        "month1 = start_datetime.month\n",
        "day1 = start_datetime.day\n",
        "\n",
        "year2 = end_datetime.year\n",
        "month2 = end_datetime.month\n",
        "day2 = end_datetime.day\n",
        "\n",
        "# Construct the URL\n",
        "url = f\"https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?network=IN__ASOS&station=VIAR&data=metar&year1={year1}&month1={month1}&day1={day1}&year2={year2}&month2={month2}&day2={day2}&tz=Etc%2FUTC&format=onlycomma&latlon=no&elev=no&missing=null&trace=0.0001&direct=no&report_type=3&report_type=4\"\n",
        "filename = \"metar_forecast_data.csv\" # Use a different filename to avoid overwriting\n",
        "response = requests.get(url)\n",
        "with open(filename, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "print(\"Data downloaded and saved as\", filename)\n",
        "\n",
        "# Load the downloaded data into a DataFrame\n",
        "df_forecast = pd.read_csv(filename)\n",
        "\n",
        "# Apply preprocessing and feature engineering steps similar to df2\n",
        "metar_attributes_fc = df_forecast['metar'].apply(parse_metar_comprehensive).tolist()\n",
        "metar_attributes_fc = [item for item in metar_attributes_fc if item is not None]\n",
        "attributes_df_fc = pd.DataFrame(metar_attributes_fc)\n",
        "df_forecast = pd.concat([df_forecast, attributes_df_fc], axis=1)\n",
        "\n",
        "sky_features_fc = df_forecast['metar'].apply(extract_sky_features).tolist()\n",
        "sky_features_df_fc = pd.DataFrame(sky_features_fc)\n",
        "df_forecast = df_forecast.drop(columns=['sky', 'cloud'], errors='ignore')\n",
        "sky_feature_columns_to_drop_fc = [col for col in df_forecast.columns if any(f'layer{i}' in col or col in ['overcast', 'nsc', 'nosig'] for i in range(1, 5))]\n",
        "df_forecast = df_forecast.drop(columns=sky_feature_columns_to_drop_fc, errors='ignore')\n",
        "df_forecast = pd.concat([df_forecast, sky_features_df_fc], axis=1)\n",
        "\n",
        "# Use the same columns_to_drop defined earlier (assuming it's in scope)\n",
        "columns_to_drop = [\n",
        "    'wind_gust',\n",
        "    'layer1_cb',\n",
        "    'layer2_cover', 'layer2_height_ft', 'layer2_cb',\n",
        "    'layer3_cover', 'layer3_height_ft', 'layer3_cb',\n",
        "    'layer4_cover', 'layer4_height_ft', 'layer4_cb',\n",
        "    'layer5_cover', 'layer5_height_ft', 'layer5_cb',\n",
        "    'ceiling_height_ft', 'layer1_cb',\n",
        "]\n",
        "df_forecast = df_forecast.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "cols_to_interpolate_linear = ['temp_c', 'dewpt_c', 'pressure_hpa', 'dewpoint_spread', 'wind_spd_kt','vis_m' ]\n",
        "for col in cols_to_interpolate_linear:\n",
        "    df_forecast[col].interpolate(method='linear', inplace=True)\n",
        "\n",
        "cols_to_fill_fb = ['has_fog', 'has_mist', 'has_haze', 'has_rain']\n",
        "for col in cols_to_fill_fb:\n",
        "    df_forecast[col].ffill(inplace=True)\n",
        "for col in cols_to_fill_fb:\n",
        "    df_forecast[col].bfill(inplace=True)\n",
        "\n",
        "df_forecast['wind_dir_deg'].fillna(360, inplace=True)\n",
        "df_forecast.loc[(df_forecast['nsc'] == 1) & (df_forecast['layer1_height_ft'].isnull()), 'layer1_height_ft'] = 25000\n",
        "df_forecast['layer1_height_ft'].interpolate(method='linear', inplace=True)\n",
        "df_forecast['flight_category'].ffill(inplace=True)\n",
        "\n",
        "# Apply lag features - make sure add_lag_features is defined earlier\n",
        "df_forecast = add_lag_features(df_forecast, ['temp_c', 'dewpt_c', 'vis_m', 'wind_spd_kt'])\n",
        "cols_to_fill_lag = [col for col in df_forecast.columns if '_lag' in col] # Dynamically get lag columns\n",
        "for col in cols_to_fill_lag:\n",
        "    df_forecast[col].bfill(inplace=True)\n",
        "    for col in cols_to_fill_lag: # Added ffill after bfill\n",
        "        df_forecast[col].ffill(inplace=True)\n",
        "\n",
        "\n",
        "# Apply cyclic features - make sure cyclic feature logic is defined earlier\n",
        "df_forecast['valid'] = pd.to_datetime(df_forecast['valid'])\n",
        "df_forecast['wind_dir_rad'] = np.deg2rad(df_forecast['wind_dir_deg'])\n",
        "df_forecast['wind_x'] = np.cos(df_forecast['wind_dir_rad'])\n",
        "df_forecast['wind_y'] = np.sin(df_forecast['wind_dir_rad'])\n",
        "\n",
        "df_forecast['hour'] = df_forecast['valid'].dt.hour\n",
        "df_forecast['minute'] = df_forecast['valid'].dt.minute\n",
        "df_forecast['hour_fraction'] = (df_forecast['hour'] + df_forecast['minute'] / 60) / 24\n",
        "df_forecast['time_x'] = np.cos(2 * np.pi * df_forecast['hour_fraction'])\n",
        "df_forecast['time_y'] = np.sin(2 * np.pi * df_forecast['hour_fraction'])\n",
        "\n",
        "df_forecast['day_of_year'] = df_forecast['valid'].dt.dayofyear\n",
        "df_forecast['day_x'] = np.cos(2 * np.pi * df_forecast['day_of_year'] / 365)\n",
        "df_forecast['day_y'] = np.sin(2 * np.pi * df_forecast['day_of_year'] / 365)\n",
        "\n",
        "print(\"\\nForecast data downloaded and preprocessed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data downloaded and saved as metar_forecast_data.csv\n",
            "\n",
            "Forecast data downloaded and preprocessed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter df_forecast to include data up to the input timestamp\n",
        "df_forecast_filtered = df_forecast[df_forecast['valid'] <= input_datetime].copy()\n",
        "\n",
        "# Calculate the timestamp 12 hours before the input datetime\n",
        "twelve_hours_ago = input_datetime - timedelta(hours=12)\n",
        "\n",
        "# Further filter to keep only the last 12 hours of data\n",
        "df_forecast_filtered = df_forecast_filtered[df_forecast_filtered['valid'] >= twelve_hours_ago].copy()\n",
        "\n",
        "print(f\"Original df_forecast shape: {df_forecast.shape}\")\n",
        "print(f\"Filtered df_forecast_filtered shape (last 12 hours): {df_forecast_filtered.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "033b59bc-9601-4427-a507-60fb96d819d1",
        "id": "nPOTUyowja1v"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original df_forecast shape: (72, 48)\n",
            "Filtered df_forecast_filtered shape (last 12 hours): (25, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Forecast"
      ],
      "metadata": {
        "id": "87Gh9UMEWR7k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e968ccde",
        "outputId": "a8f6373e-c6de-4ccf-f6a6-1373f0995ff0"
      },
      "source": [
        "# Ensure df_test_filtered has at least enough rows for the maximum lag (6 in this case)\n",
        "if len(df_forecast_filtered) > 6:\n",
        "    # Define the features used during training\n",
        "    features_for_prediction = ['wind_spd_kt', 'pressure_hpa', 'temp_c_lag1', 'temp_c_lag2', 'temp_c_lag3', 'temp_c_lag6',\n",
        "                'dewpt_c_lag1', 'dewpt_c_lag2', 'dewpt_c_lag3', 'dewpt_c_lag6', 'time_x', 'time_y', 'day_of_year', 'day_x', 'day_y']\n",
        "\n",
        "    forecasted_temps = forecast_temperature(rf_final_temp, df_forecast_filtered, features_for_prediction, horizon=2)\n",
        "\n",
        "    # Calculate the forecast timestamp (1 hour after the last timestamp in filtered data)\n",
        "    last_timestamp = df_forecast_filtered['valid'].iloc[-1]\n",
        "    forecast_timestamp = last_timestamp + timedelta(hours=1)\n",
        "\n",
        "    print(f\"\\nForecast for {forecast_timestamp}: {forecasted_temps[-1]:.2f} °C\")\n",
        "else:\n",
        "    print(\"df_test_filtered does not have enough data points for forecasting with the specified lags.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forecast for 2025-09-10 18:30:00: 27.04 °C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the last two rows of the filtered forecast DataFrame\n",
        "last_two_rows = df_forecast_filtered.tail(3)\n",
        "\n",
        "# Print the valid timestamp and temp_c for these rows\n",
        "print(\"Last two temperature values from filtered forecast data:\")\n",
        "for index, row in last_two_rows.iterrows():\n",
        "    print(f\"Time: {row['valid']}, Temperature: {row['temp_c']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-pEOEpPVX9v",
        "outputId": "f1e7060f-e3ff-4677-9259-df9aa0e0c017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last two temperature values from filtered forecast data:\n",
            "Time: 2025-09-10 16:30:00, Temperature: 27.0\n",
            "Time: 2025-09-10 17:00:00, Temperature: 27.0\n",
            "Time: 2025-09-10 17:30:00, Temperature: 27.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure df_test_filtered has at least enough rows for the maximum lag (6 in this case)\n",
        "if len(df_forecast_filtered) > 6:\n",
        "    # Define the features used during training\n",
        "    features_for_prediction = ['wind_spd_kt', 'pressure_hpa', 'temp_c_lag1', 'temp_c_lag2', 'temp_c_lag3', 'temp_c_lag6',\n",
        "                'dewpt_c_lag1', 'dewpt_c_lag2', 'dewpt_c_lag3', 'dewpt_c_lag6', 'time_x', 'time_y', 'day_of_year', 'day_x', 'day_y']\n",
        "\n",
        "    # Forecast using Random Forest model\n",
        "    rf_forecasted_temps = forecast_temperature(rf_final_temp, df_forecast_filtered, features_for_prediction, horizon=2)\n",
        "\n",
        "    # Forecast using XGBoost model\n",
        "    xgb_forecasted_temps = forecast_temperature(xgb_final_temp, df_forecast_filtered, features_for_prediction, horizon=2)\n",
        "\n",
        "\n",
        "    # Calculate the forecast timestamp (1 hour after the last timestamp in filtered data)\n",
        "    last_timestamp = df_forecast_filtered['valid'].iloc[-1]\n",
        "    forecast_timestamp = last_timestamp + timedelta(hours=1)\n",
        "\n",
        "    print(f\"\\nRandom Forest Forecast for {forecast_timestamp}: {rf_forecasted_temps[-1]:.2f} °C\")\n",
        "    print(f\"XGBoost Forecast for {forecast_timestamp}: {xgb_forecasted_temps[-1]:.2f} °C\")\n",
        "else:\n",
        "    print(\"df_test_filtered does not have enough data points for forecasting with the specified lags.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO6n97WZlxhX",
        "outputId": "3bb774af-b4a0-46f2-b3d2-cba7f68f82c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Forest Forecast for 2025-09-10 18:30:00: 27.04 °C\n",
            "XGBoost Forecast for 2025-09-10 18:30:00: 23.89 °C\n"
          ]
        }
      ]
    }
  ]
}